#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨å›½æ ‡å‡†ä¿¡æ¯å…¬å…±æœåŠ¡å¹³å°çˆ¬è™« v2.0
æ–°å¢åŠŸèƒ½ï¼šè¯¦æƒ…è·å–ã€PDFä¸‹è½½é“¾æ¥ã€æ‰¹é‡æœç´¢
"""

import asyncio
import json
import csv
import re
import random
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Callable
from playwright.async_api import async_playwright, Page, Browser

# è¿›åº¦å›è°ƒç±»å‹
ProgressCallback = Callable[[int, str], None]


class StdCrawler:
    """æ ‡å‡†ä¿¡æ¯çˆ¬è™«ç±»"""

    BASE_URL = "https://std.samr.gov.cn"
    SEARCH_URL = "https://std.samr.gov.cn/search/std"
    OPENSTD_URL = "https://openstd.samr.gov.cn"

    def __init__(self, headless: bool = True, delay: float = 1.0):
        """
        åˆå§‹åŒ–çˆ¬è™«
        """
        self.headless = headless
        self.delay = delay
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.results = []

    async def start(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=self.headless)
        self.page = await self.browser.new_page()
        self.page.set_default_timeout(60000)  # å¢åŠ åˆ°60ç§’
        print("æµè§ˆå™¨å·²å¯åŠ¨")

    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
            print("æµè§ˆå™¨å·²å…³é—­")

    async def batch_search(self, keywords: List[str], max_pages: int = 3,
                           std_type: str = "å…¨éƒ¨", std_status: str = "å…¨éƒ¨",
                           get_details: bool = False,
                           progress_callback: Optional[ProgressCallback] = None) -> List[Dict]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (progress_percent, message)
        """
        all_results = []
        total_keywords = len(keywords)

        def report_progress(percent: int, msg: str):
            """æŠ¥å‘Šè¿›åº¦"""
            print(msg)
            if progress_callback:
                progress_callback(percent, msg)

        for i, keyword in enumerate(keywords):
            # è®¡ç®—æœç´¢é˜¶æ®µè¿›åº¦ (0-50% ç”¨äºæœç´¢)
            search_base = int((i / total_keywords) * 50)
            report_progress(search_base, f"ğŸ” [{i+1}/{total_keywords}] æ­£åœ¨æœç´¢: {keyword}")

            results = await self.search(
                keyword=keyword,
                max_pages=max_pages,
                std_type=std_type,
                std_status=std_status
            )

            report_progress(search_base + 5, f"âœ… [{i+1}/{total_keywords}] {keyword}: æ‰¾åˆ° {len(results)} æ¡è®°å½•")

            if get_details and results:
                # è¯¦æƒ…è·å–é˜¶æ®µ (50-95%)
                max_details = min(len(results), 20)
                detail_base = 50 + int((i / total_keywords) * 45)

                for j, result in enumerate(results[:max_details]):
                    if result.get("url"):
                        detail_progress = detail_base + int((j / max_details) * (45 / total_keywords))
                        std_code = result.get('std_code', 'æœªçŸ¥')
                        report_progress(
                            detail_progress,
                            f"ğŸ“„ [{i+1}/{total_keywords}] è·å–è¯¦æƒ… ({j+1}/{max_details}): {std_code}"
                        )
                        detail = await self.get_detail(result["url"])
                        result.update(detail)
                        await asyncio.sleep(self.delay + random.uniform(0.5, 1.5))

            for result in results:
                result["search_keyword"] = keyword

            all_results.extend(results)

            if i < len(keywords) - 1:
                await asyncio.sleep(self.delay)

        self.results = all_results
        report_progress(100, f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_results)} æ¡è®°å½•")
        return all_results

    async def search(self, keyword: str, max_pages: int = 5,
                     std_type: str = "å…¨éƒ¨", std_status: str = "å…¨éƒ¨") -> list:
        """æœç´¢æ ‡å‡†"""
        self.results = []

        search_url = f"{self.SEARCH_URL}?q={keyword}"
        print(f"\nå¼€å§‹æœç´¢: {keyword}")

        await self.page.goto(search_url)
        await self.page.wait_for_load_state("networkidle")
        await asyncio.sleep(2)

        if std_type != "å…¨éƒ¨":
            await self._select_filter("æ ‡å‡†ç±»å‹", std_type)
        if std_status != "å…¨éƒ¨":
            await self._select_filter("æ ‡å‡†çŠ¶æ€", std_status)

        total_text = await self._get_total_count()
        print(f"æ‰¾åˆ°ç»“æœ: {total_text}")

        for page_num in range(1, max_pages + 1):
            print(f"\n--- æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ ---")

            page_results = await self._parse_search_results()

            if not page_results:
                print("æ²¡æœ‰æ›´å¤šç»“æœ")
                break

            self.results.extend(page_results)
            print(f"æœ¬é¡µè·å– {len(page_results)} æ¡è®°å½•")

            has_next = await self._has_next_page()
            if not has_next or page_num >= max_pages:
                break

            await self._goto_next_page()
            delay = self.delay + random.uniform(0, 1)
            await asyncio.sleep(delay)

        print(f"\nçˆ¬å–å®Œæˆï¼Œå…±è·å– {len(self.results)} æ¡è®°å½•")
        return self.results

    async def _select_filter(self, filter_name: str, value: str):
        """é€‰æ‹©ç­›é€‰æ¡ä»¶"""
        try:
            selector = f'text="{value}"'
            await self.page.click(selector)
            await asyncio.sleep(1)
        except Exception as e:
            print(f"ç­›é€‰æ¡ä»¶è®¾ç½®å¤±è´¥: {e}")

    async def _get_total_count(self) -> str:
        """è·å–æœç´¢ç»“æœæ€»æ•°"""
        try:
            iframe = self.page.frame_locator("iframe").first
            total_element = iframe.locator("text=ä¸ºæ‚¨æ‰¾åˆ°ç›¸å…³ç»“æœçº¦")
            text = await total_element.text_content()
            return text if text else "æœªçŸ¥"
        except Exception:
            return "æœªçŸ¥"

    async def _parse_search_results(self) -> list:
        """è§£ææœç´¢ç»“æœ"""
        results = []

        try:
            iframe = self.page.frame_locator("iframe").first
            items = iframe.locator("table").filter(has=iframe.locator("a[href*='Detailed']"))
            count = await items.count()

            for i in range(count):
                try:
                    item = items.nth(i)

                    link_element = item.locator("a[href*='Detailed']").first
                    title = await link_element.text_content()
                    href = await link_element.get_attribute("href")

                    if not title or not href:
                        continue

                    result = self._parse_title(title.strip())
                    result["url"] = href if href.startswith("http") else f"{self.BASE_URL}{href}"
                    result["title"] = title.strip()

                    try:
                        status_element = item.locator("td").last
                        status = await status_element.text_content()
                        result["status"] = status.strip() if status else ""
                    except:
                        result["status"] = ""

                    results.append(result)

                except Exception:
                    continue

        except Exception as e:
            print(f"è§£æç»“æœå‡ºé”™: {e}")

        return results

    def _parse_title(self, title: str) -> dict:
        """è§£ææ ‡å‡†æ ‡é¢˜"""
        title = re.sub(r'\s+', ' ', title).strip()
        parts = title.split(" ", 1)
        if len(parts) == 2:
            return {"std_code": parts[0], "std_name": parts[1]}
        return {"std_code": "", "std_name": title}

    async def _has_next_page(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ"""
        try:
            iframe = self.page.frame_locator("iframe").first
            next_btn = iframe.locator("text=ä¸‹ä¸€é¡µ")
            return await next_btn.count() > 0
        except:
            return False

    async def _goto_next_page(self):
        """è·³è½¬åˆ°ä¸‹ä¸€é¡µ"""
        try:
            iframe = self.page.frame_locator("iframe").first
            next_btn = iframe.locator("text=ä¸‹ä¸€é¡µ")
            await next_btn.click()
            await self.page.wait_for_load_state("networkidle")
            await asyncio.sleep(1)
        except Exception as e:
            print(f"ç¿»é¡µå¤±è´¥: {e}")

    async def get_detail(self, url: str, retry_count: int = 2) -> dict:
        """è·å–æ ‡å‡†è¯¦æƒ…ï¼Œæ”¯æŒé‡è¯•"""
        detail = {}

        for attempt in range(retry_count + 1):
            try:
                # ä½¿ç”¨è¾ƒé•¿è¶…æ—¶ï¼Œåªç­‰å¾…domcontentloadedè€Œéå®Œæ•´åŠ è½½
                await self.page.goto(url, timeout=45000, wait_until="domcontentloaded")
                await asyncio.sleep(2)  # ç­‰å¾…é¡µé¢æ¸²æŸ“

                # è·å–æ ‡å‡†åç§°
                try:
                    cn_title = await self.page.locator("h4").first.text_content(timeout=5000)
                    detail["cn_title"] = cn_title.strip() if cn_title else ""
                except:
                    pass

                try:
                    en_title = await self.page.locator("h5").first.text_content(timeout=5000)
                    detail["en_title"] = en_title.strip() if en_title else ""
                except:
                    pass

                # è·å–åŸºç¡€ä¿¡æ¯
                try:
                    terms = await self.page.locator("dt").all()
                    definitions = await self.page.locator("dd").all()

                    for i, term in enumerate(terms):
                        if i < len(definitions):
                            key = await term.text_content(timeout=3000)
                            value = await definitions[i].text_content(timeout=3000)
                            if key and value:
                                key = key.strip().replace("ï¼š", "").replace(":", "")
                                value = value.strip()
                                detail[key] = value
                except:
                    pass

                # è·å–èµ·è‰å•ä½
                try:
                    paragraph = await self.page.locator("p:has-text('ä¸»è¦èµ·è‰å•ä½')").text_content(timeout=5000)
                    if paragraph:
                        units = paragraph.replace("ä¸»è¦èµ·è‰å•ä½", "").strip()
                        detail["èµ·è‰å•ä½"] = units
                except:
                    pass

                # è·å–èµ·è‰äºº
                try:
                    paragraph = await self.page.locator("p:has-text('ä¸»è¦èµ·è‰äºº')").text_content(timeout=5000)
                    if paragraph:
                        persons = paragraph.replace("ä¸»è¦èµ·è‰äºº", "").strip()
                        detail["èµ·è‰äºº"] = persons
                except:
                    pass

                # åªæœ‰æˆåŠŸè·å–åˆ°åŸºæœ¬ä¿¡æ¯æ‰å°è¯•è·å–PDFé“¾æ¥
                if detail.get("cn_title") or detail.get("æ ‡å‡†å·"):
                    try:
                        pdf_info = await self._get_pdf_link()
                        if pdf_info:
                            detail.update(pdf_info)
                    except Exception as e:
                        print(f"è·å–PDFé“¾æ¥å¤±è´¥: {e}")

                # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                if detail:
                    break

            except Exception as e:
                print(f"è·å–è¯¦æƒ…å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count + 1}): {e}")
                if attempt < retry_count:
                    wait_time = (attempt + 1) * 3  # é€’å¢ç­‰å¾…æ—¶é—´
                    print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)

        return detail

    async def _get_pdf_link(self) -> dict:
        """è·å–æ ‡å‡†PDFä¸‹è½½é“¾æ¥"""
        pdf_info = {}

        try:
            view_text_btn = self.page.locator("text=æŸ¥çœ‹æ–‡æœ¬").first
            if await view_text_btn.count() > 0:
                async with self.page.context.expect_page() as new_page_info:
                    await view_text_btn.click()
                    await asyncio.sleep(2)

                try:
                    new_page = await new_page_info.value
                    await new_page.wait_for_load_state("networkidle")

                    pdf_page_url = new_page.url
                    pdf_info["pdf_page_url"] = pdf_page_url

                    download_btn = new_page.locator("button:has-text('ä¸‹è½½æ ‡å‡†')")
                    if await download_btn.count() > 0:
                        pdf_info["has_pdf_download"] = True

                    preview_btn = new_page.locator("button:has-text('åœ¨çº¿é¢„è§ˆ')")
                    if await preview_btn.count() > 0:
                        pdf_info["has_online_preview"] = True

                    hcno_match = re.search(r'hcno=([A-F0-9]+)', pdf_page_url)
                    if hcno_match:
                        hcno = hcno_match.group(1)
                        pdf_info["hcno"] = hcno
                        pdf_info["pdf_download_url"] = f"https://openstd.samr.gov.cn/bzgk/std/downLoadView?hcno={hcno}"

                    await new_page.close()
                except:
                    pass

        except Exception:
            pass

        return pdf_info

    def save_to_csv(self, filename: str = None):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        if not self.results:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"std_results_{timestamp}.csv"

        filepath = Path(filename)

        fieldnames = set()
        for result in self.results:
            fieldnames.update(result.keys())
        fieldnames = sorted(list(fieldnames))

        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.results)

        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath.absolute()}")

    def save_to_json(self, filename: str = None):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        if not self.results:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"std_results_{timestamp}.json"

        filepath = Path(filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath.absolute()}")


async def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    crawler = StdCrawler(headless=True, delay=1.5)

    try:
        await crawler.start()

        # æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
        keywords = ["å®‰å…¨ç”Ÿäº§", "æ¥åœ°æ ‡å‡†"]
        results = await crawler.batch_search(
            keywords=keywords,
            max_pages=2,
            get_details=True
        )

        print("\n=== ç»“æœé¢„è§ˆ ===")
        for i, result in enumerate(results[:5], 1):
            print(f"{i}. {result.get('std_code', '')} - {result.get('std_name', '')}")
            print(f"   çŠ¶æ€: {result.get('status', '')} | å…³é”®è¯: {result.get('search_keyword', '')}")
            if result.get('pdf_page_url'):
                print(f"   PDFé¡µé¢: {result.get('pdf_page_url', '')}")

        crawler.save_to_csv()
        crawler.save_to_json()

    finally:
        await crawler.close()


if __name__ == "__main__":
    asyncio.run(main())
