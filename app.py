#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FastAPI åç«¯æœåŠ¡ v2.0
æ–°å¢ï¼šæ‰¹é‡æœç´¢ã€è¯¦æƒ…è·å–ã€PDFé“¾æ¥
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Optional, List
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from std_crawler import StdCrawler

app = FastAPI(
    title="æ ‡å‡†ä¿¡æ¯çˆ¬è™« API",
    description="å…¨å›½æ ‡å‡†ä¿¡æ¯å…¬å…±æœåŠ¡å¹³å°çˆ¬è™«æ¥å£ v2.0",
    version="2.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å­˜å‚¨çˆ¬å–ä»»åŠ¡çŠ¶æ€
tasks_status = {}

# æ•°æ®å­˜å‚¨ç›®å½•
DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚å‚æ•°"""
    keyword: str
    max_pages: int = 3
    std_type: str = "å…¨éƒ¨"
    std_status: str = "å…¨éƒ¨"
    get_details: bool = False


class BatchSearchRequest(BaseModel):
    """æ‰¹é‡æœç´¢è¯·æ±‚å‚æ•°"""
    keywords: List[str]
    max_pages: int = 2
    std_type: str = "å…¨éƒ¨"
    std_status: str = "å…¨éƒ¨"
    get_details: bool = False


class TaskResponse(BaseModel):
    """ä»»åŠ¡å“åº”"""
    task_id: str
    status: str
    message: str


@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„"""
    return {
        "name": "æ ‡å‡†ä¿¡æ¯çˆ¬è™« API",
        "version": "2.0.0",
        "endpoints": {
            "search": "/api/search",
            "batch_search": "/api/batch-search",
            "status": "/api/status/{task_id}",
            "results": "/api/results/{task_id}",
            "download": "/api/download/{task_id}"
        }
    }


@app.post("/api/search", response_model=TaskResponse)
async def start_search(request: SearchRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨å•å…³é”®è¯çˆ¬å–ä»»åŠ¡"""
    task_id = datetime.now().strftime("%Y%m%d_%H%M%S")

    tasks_status[task_id] = {
        "status": "running",
        "progress": 0,
        "message": "æ­£åœ¨å¯åŠ¨çˆ¬è™«...",
        "keyword": request.keyword,
        "keywords": [request.keyword],
        "results": [],
        "total": 0,
        "created_at": datetime.now().isoformat(),
        "get_details": request.get_details
    }

    background_tasks.add_task(
        run_crawler,
        task_id,
        [request.keyword],
        request.max_pages,
        request.std_type,
        request.std_status,
        request.get_details
    )

    return TaskResponse(
        task_id=task_id,
        status="running",
        message=f"çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå…³é”®è¯: {request.keyword}"
    )


@app.post("/api/batch-search", response_model=TaskResponse)
async def start_batch_search(request: BatchSearchRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨æ‰¹é‡æœç´¢ä»»åŠ¡"""
    task_id = datetime.now().strftime("%Y%m%d_%H%M%S")

    keywords_str = ", ".join(request.keywords[:3])
    if len(request.keywords) > 3:
        keywords_str += f" ç­‰{len(request.keywords)}ä¸ª"

    tasks_status[task_id] = {
        "status": "running",
        "progress": 0,
        "message": f"æ­£åœ¨æ‰¹é‡æœç´¢: {keywords_str}",
        "keyword": keywords_str,
        "keywords": request.keywords,
        "results": [],
        "total": 0,
        "created_at": datetime.now().isoformat(),
        "get_details": request.get_details
    }

    background_tasks.add_task(
        run_crawler,
        task_id,
        request.keywords,
        request.max_pages,
        request.std_type,
        request.std_status,
        request.get_details
    )

    return TaskResponse(
        task_id=task_id,
        status="running",
        message=f"æ‰¹é‡çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå…± {len(request.keywords)} ä¸ªå…³é”®è¯"
    )


async def run_crawler(task_id: str, keywords: List[str], max_pages: int,
                      std_type: str, std_status: str, get_details: bool):
    """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
    crawler = StdCrawler(headless=True, delay=1.5)

    def update_progress(progress: int, message: str):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        tasks_status[task_id]["progress"] = progress
        tasks_status[task_id]["message"] = message

    try:
        update_progress(5, "ğŸš€ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        await crawler.start()

        update_progress(10, f"ğŸ” å¼€å§‹æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯...")

        # ä½¿ç”¨æ‰¹é‡æœç´¢ï¼Œä¼ å…¥è¿›åº¦å›è°ƒ
        results = await crawler.batch_search(
            keywords=keywords,
            max_pages=max_pages,
            std_type=std_type,
            std_status=std_status,
            get_details=get_details,
            progress_callback=update_progress
        )

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        tasks_status[task_id]["status"] = "completed"
        tasks_status[task_id]["message"] = f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(results)} æ¡è®°å½•"
        tasks_status[task_id]["results"] = results
        tasks_status[task_id]["total"] = len(results)
        tasks_status[task_id]["progress"] = 100

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        result_file = DATA_DIR / f"results_{task_id}.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        tasks_status[task_id]["file"] = str(result_file)

    except Exception as e:
        tasks_status[task_id]["status"] = "failed"
        tasks_status[task_id]["message"] = f"çˆ¬å–å¤±è´¥: {str(e)}"

    finally:
        await crawler.close()


@app.get("/api/status/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    task = tasks_status[task_id]
    return {
        "task_id": task_id,
        "status": task["status"],
        "progress": task["progress"],
        "message": task["message"],
        "total": task.get("total", 0),
        "keyword": task.get("keyword", ""),
        "keywords": task.get("keywords", []),
        "get_details": task.get("get_details", False)
    }


@app.get("/api/results/{task_id}")
async def get_task_results(task_id: str, page: int = 1, page_size: int = 20):
    """è·å–çˆ¬å–ç»“æœ"""
    if task_id not in tasks_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    task = tasks_status[task_id]
    results = task.get("results", [])

    # åˆ†é¡µ
    total = len(results)
    start = (page - 1) * page_size
    end = start + page_size
    paginated_results = results[start:end]

    return {
        "task_id": task_id,
        "status": task["status"],
        "total": total,
        "page": page,
        "page_size": page_size,
        "total_pages": (total + page_size - 1) // page_size,
        "results": paginated_results
    }


@app.get("/api/download/{task_id}")
async def download_results(task_id: str, format: str = "json"):
    """ä¸‹è½½çˆ¬å–ç»“æœ"""
    if task_id not in tasks_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    task = tasks_status[task_id]
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")

    results = task.get("results", [])

    if format == "json":
        filename = f"standards_{task_id}.json"
        filepath = DATA_DIR / filename
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        return FileResponse(
            filepath,
            media_type="application/json",
            filename=filename
        )

    elif format == "csv":
        import csv
        filename = f"standards_{task_id}.csv"
        filepath = DATA_DIR / filename

        if results:
            fieldnames = set()
            for r in results:
                fieldnames.update(r.keys())
            fieldnames = sorted(list(fieldnames))

            with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(results)

        return FileResponse(
            filepath,
            media_type="text/csv",
            filename=filename
        )

    else:
        raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ ¼å¼")


@app.get("/api/history")
async def get_history():
    """è·å–å†å²ä»»åŠ¡åˆ—è¡¨"""
    history = []
    for task_id, task in tasks_status.items():
        history.append({
            "task_id": task_id,
            "keyword": task.get("keyword", ""),
            "keywords": task.get("keywords", []),
            "status": task["status"],
            "total": task.get("total", 0),
            "created_at": task.get("created_at", ""),
            "get_details": task.get("get_details", False)
        })

    history.sort(key=lambda x: x["created_at"], reverse=True)
    return {"history": history}


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
